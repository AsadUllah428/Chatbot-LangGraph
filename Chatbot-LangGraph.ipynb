{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a1390e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chatbot initialized. Type 'quit', 'Quit' or 'exit' to exit.\n",
      "\n",
      "Assistant: Hello! It's nice to meet you. How can I assist you today?\n",
      "\n",
      "Assistant: I'm here to help. Is there something on your mind that you'd like to talk about or ask?\n",
      "\n",
      "Assistant: I'm doing well, thank you for asking! I'm a large language model, so I don't have feelings like humans do, but I'm always happy to chat with you and help with any questions you might have.\n",
      "\n",
      "Assistant: Let's start fresh! Is there something specific you'd like to talk about, ask, or learn about today?\n",
      "\n",
      "Assistant: Stars are massive balls of hot, glowing gas in space, primarily composed of hydrogen and helium. They are held together by their own gravity and sustained by nuclear reactions in their cores. There are billions of stars in the universe, ranging in size, color, and temperature.\n",
      "\n",
      "Assistant: The largest known star in the universe is VY Canis Majoris, a red hypergiant located in the constellation Canis Major. It's about 2,100 times larger than our sun, with a radius of approximately 2,000 solar radii (1,420,000,000 kilometers or 880,000,000 miles).\n",
      "\n",
      "Assistant: The largest planet in our solar system is Jupiter. It's a gas giant, with a diameter of approximately 142,984 kilometers (88,846 miles), which is more than 11 times the diameter of the Earth.\n",
      "\n",
      "Assistant: Goodbye!\n"
     ]
    }
   ],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "from typing import Annotated, List, Dict, Any\n",
    "from typing_extensions import TypedDict\n",
    "from langgraph.graph import StateGraph, END, START\n",
    "from langgraph.graph.message import add_messages\n",
    "from langchain_core.messages import HumanMessage, AIMessage, SystemMessage\n",
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "import os\n",
    "import signal\n",
    "import sys\n",
    "\n",
    "# Configuration\n",
    "os.environ[\"GROQ_API_KEY\"] = \"your-groq-api-key-here\"  # your-groq-api-key-here\n",
    "\n",
    "# Initialize LLM - using LLaMA3\n",
    "llm = ChatGroq(\n",
    "    temperature=0.7,\n",
    "    model_name=\"your-desired-model-name\",     # your-desired-model-name\"\n",
    "    groq_api_key=os.getenv(\"GROQ_API_KEY\")\n",
    ")\n",
    "\n",
    "# Memory\n",
    "memory = ConversationBufferWindowMemory(k=6, return_messages=True)\n",
    "\n",
    "# System prompt\n",
    "SYSTEM_PROMPT = \"\"\"You are an expert AI assistant. Be helpful, concise, and friendly. \n",
    "Respond professionally without revealing your internal thinking process.\n",
    "Keep responses under 3 sentences unless detailed explanation is needed.\"\"\"\n",
    "\n",
    "# State definition\n",
    "class State(TypedDict):\n",
    "    messages: Annotated[List[Dict[str, Any]], add_messages]\n",
    "    should_end: bool\n",
    "\n",
    "# Signal handler for graceful exit\n",
    "def signal_handler(sig, frame):\n",
    "    print(\"\\nAssistant: Goodbye!\")\n",
    "    sys.exit(0)\n",
    "\n",
    "signal.signal(signal.SIGINT, signal_handler)\n",
    "\n",
    "# Nodes\n",
    "def get_user_input(state: State):\n",
    "    user_input = input(\"\\nUser: \")\n",
    "    if user_input.lower() in [\"quit\", \"exit\"]:\n",
    "        return {\"messages\": [SystemMessage(content=\"SESSION_END\")], \"should_end\": True}\n",
    "    return {\"messages\": [HumanMessage(content=user_input)], \"should_end\": False}\n",
    "\n",
    "def chatbot(state: State):\n",
    "    try:\n",
    "        # Check if we should end first\n",
    "        if state.get(\"should_end\", False):\n",
    "            return state\n",
    "            \n",
    "        # Prepare conversation history\n",
    "        chat_history = [SystemMessage(content=SYSTEM_PROMPT)]\n",
    "        chat_history.extend(memory.load_memory_variables({})[\"history\"] if memory.chat_memory.messages else [])\n",
    "        chat_history.extend(state['messages'])\n",
    "        \n",
    "        # Get response\n",
    "        response = llm.invoke(chat_history)\n",
    "        \n",
    "        # Update memory\n",
    "        if len(state['messages']) > 0 and isinstance(state['messages'][-1], HumanMessage):\n",
    "            memory.save_context(\n",
    "                {\"input\": state['messages'][-1].content},\n",
    "                {\"output\": response.content}\n",
    "            )\n",
    "        \n",
    "        return {\"messages\": [AIMessage(content=response.content)], \"should_end\": False}\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        return {\"messages\": [AIMessage(content=\"Sorry, I encountered an error. Please try again.\")], \"should_end\": False}\n",
    "\n",
    "# Graph construction\n",
    "def should_continue(state: State):\n",
    "    # Check if there's a SESSION_END message or the should_end flag is True\n",
    "    if state.get(\"should_end\", False):\n",
    "        return END\n",
    "        \n",
    "    # Check for SESSION_END message\n",
    "    if len(state['messages']) > 0 and isinstance(state['messages'][-1], SystemMessage):\n",
    "        if state['messages'][-1].content == \"SESSION_END\":\n",
    "            return END\n",
    "            \n",
    "    return \"get_input\"\n",
    "\n",
    "# Build workflow\n",
    "workflow = StateGraph(State)\n",
    "workflow.add_node(\"get_input\", get_user_input)\n",
    "workflow.add_node(\"chatbot\", chatbot)\n",
    "workflow.add_edge(START, \"get_input\")\n",
    "workflow.add_edge(\"get_input\", \"chatbot\")\n",
    "workflow.add_conditional_edges(\n",
    "    \"chatbot\",\n",
    "    should_continue,\n",
    "    {\n",
    "        \"get_input\": \"get_input\",\n",
    "        END: END\n",
    "    }\n",
    ")\n",
    "graph = workflow.compile()\n",
    "\n",
    "# Main loop\n",
    "def main():\n",
    "    print(\"Chatbot initialized. Type 'quit', 'Quit' or 'exit' to exit.\")\n",
    "    \n",
    "    try:\n",
    "        for event in graph.stream({\"messages\": [], \"should_end\": False}):\n",
    "            for key, value in event.items():\n",
    "                if key == \"chatbot\" and \"messages\" in value and len(value['messages']) > 0:\n",
    "                    last_msg = value['messages'][-1]\n",
    "                    if isinstance(last_msg, AIMessage):\n",
    "                        print(f\"\\nAssistant: {last_msg.content}\")\n",
    "                \n",
    "                # Check if we should end\n",
    "                if value.get(\"should_end\", False):\n",
    "                    print(\"\\nAssistant: Goodbye!\")\n",
    "                    return\n",
    "    except StopIteration:\n",
    "        print(\"\\nAssistant: Goodbye!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
